{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3587aec-54a7-42ad-9d35-3eb04d64dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1.post300)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from imageio) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.11/site-packages (from imageio) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.11.0.86\n",
      "Requirement already satisfied: imageio[ffmpeg] in /opt/conda/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (11.1.0)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
      "  Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (5.9.8)\n",
      "Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "Installing collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tuesday Feb 25 \n",
    "\n",
    "@author: Dr. Benjamin Vien\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## TO INSTALL\n",
    "# pip install torch torchvision imageio matplotlib opencv-python\n",
    "!pip install torch torchvision\n",
    "!pip install imageio\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python-headless\n",
    "!pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a788f33-fa7c-47ba-809b-b9a37302c06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 4\n",
      "CUDA device name: NVIDIA A10G\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" \n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time\n",
    "from syntheticdatageneration_utils import SyntheticDataset,print_model_summary,train_finetune_updated,fetch_optimizer,generate_interpolated_video\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ab7d7c-6b61-42f6-b93d-af6bb04c1a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Loading video...\n",
      "Video loaded. Using frames 170 to 300 with step 2.\n",
      "Loading MATLAB query points...\n",
      "Original MATLAB query points shape: torch.Size([1024, 2, 483])\n",
      "Clipped MATLAB query points shape: torch.Size([1024, 2, 65])\n",
      "Base query points shape: (1024, 2)\n",
      "Model Loaded!\n",
      "Reloaded previous fine-tuned model.\n",
      "Total number of top-level layers: 1\n",
      "Layer 0: model\n",
      "Starting Fine-Tuning...\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1292/1461908467.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"./model_saved/cotracker3_finetuned_AWS_v1SYN_fullFNET_100samplesEPOCH20.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Autocast] [Batch 1] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 2] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 3] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 4] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 5] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 6] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 7] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 8] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 9] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 10] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 11] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 12] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 13] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 14] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 15] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 16] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 17] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 18] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 19] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 20] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 21] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 22] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 23] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 24] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 25] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 26] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 27] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 28] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 29] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 30] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 31] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 32] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 33] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 34] pred_tracks.dtype: torch.float32\n",
      "Epoch [1/5], Loss: 4.7932\n",
      "\n",
      "Epoch: 2\n",
      "  [Autocast] [Batch 1] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 2] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 3] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 4] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 5] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 6] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 7] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 8] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 9] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 10] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 11] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 12] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 13] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 14] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 15] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 16] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 17] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 18] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 19] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 20] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 21] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 22] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 23] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 24] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 25] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 26] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 27] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 28] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 29] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 30] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 31] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 32] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 33] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 34] pred_tracks.dtype: torch.float32\n",
      "Epoch [2/5], Loss: 4.4867\n",
      "\n",
      "Epoch: 3\n",
      "  [Autocast] [Batch 1] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 2] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 3] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 4] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 5] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 6] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 7] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 8] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 9] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 10] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 11] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 12] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 13] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 14] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 15] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 16] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 17] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 18] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 19] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 20] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 21] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 22] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 23] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 24] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 25] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 26] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 27] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 28] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 29] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 30] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 31] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 32] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 33] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 34] pred_tracks.dtype: torch.float32\n",
      "Epoch [3/5], Loss: 3.6999\n",
      "\n",
      "Epoch: 4\n",
      "  [Autocast] [Batch 1] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 2] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 3] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 4] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 5] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 6] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 7] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 8] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 9] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 10] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 11] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 12] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 13] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 14] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 15] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 16] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 17] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 18] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 19] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 20] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 21] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 22] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 23] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 24] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 25] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 26] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 27] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 28] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 29] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 30] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 31] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 32] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 33] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 34] pred_tracks.dtype: torch.float32\n",
      "Epoch [4/5], Loss: 3.6685\n",
      "\n",
      "Epoch: 5\n",
      "  [Autocast] [Batch 1] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 2] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 3] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 4] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 5] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 6] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 7] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 8] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 9] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 10] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 11] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 12] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 13] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 14] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 15] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 16] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 17] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 18] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 19] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 20] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 21] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 22] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 23] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 24] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 25] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 26] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 27] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 28] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 29] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 30] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 31] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 32] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 33] pred_tracks.dtype: torch.float32\n",
      "  [Autocast] [Batch 34] pred_tracks.dtype: torch.float32\n",
      "Epoch [5/5], Loss: 3.3638\n",
      "Elapsed time: 264.920386 seconds\n",
      "--------------------------------------------------------------\n",
      "Fine-tuning completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Main Fine-Tuning Script with Synthetic Data\n",
    "# -----------------------------\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Training on device:\", device)\n",
    "    \n",
    "    # User-defined parameters for the video clip.\n",
    "    clip_start = 170\n",
    "    clip_end = 300\n",
    "    step = 2\n",
    "    num_aug_frames = 4\n",
    "    # Number of synthetic frames per sequence.\n",
    "    \n",
    "    video_path = \"./Input Files/IMG_7296.mp4\"\n",
    "    mat_path = \"./matlab_files/saved_objC_ordered_NaN_fixed.mat\"\n",
    "    \n",
    "    print(\"Loading video...\")\n",
    "    frames = iio.imread(video_path, plugin='FFMPEG')\n",
    "    frames_clip = frames[clip_start:clip_end:step]\n",
    "    print(\"Video loaded. Using frames {} to {} with step {}.\".format(clip_start, clip_end, step))\n",
    "    \n",
    "    # Use the first frame of the clip as the base image.\n",
    "    base_image = frames_clip[0]\n",
    "    \n",
    "    print(\"Loading MATLAB query points...\")\n",
    "    mat_data = sio.loadmat(mat_path)\n",
    "    saved_objC = mat_data['data']  # Expected shape: (N, 2, Total_Frames)\n",
    "    saved_objC_tensor = torch.tensor(saved_objC, dtype=torch.float32)\n",
    "    saved_objC_tensor[:, :2, :] = saved_objC_tensor[:, :2, :] - 1\n",
    "    print(\"Original MATLAB query points shape:\", saved_objC_tensor.shape)\n",
    "    \n",
    "    # Clip query points to match video clip.\n",
    "    saved_objC_tensor_clipped = saved_objC_tensor[:, :, clip_start:clip_end:step]\n",
    "    print(\"Clipped MATLAB query points shape:\", saved_objC_tensor_clipped.shape)\n",
    "    \n",
    "    # Extract base query points for the first frame (shape: (N,2)).\n",
    "    base_query_points = saved_objC_tensor_clipped[:, :, 0].cpu().numpy()\n",
    "    print(\"Base query points shape:\", base_query_points.shape)\n",
    "    \n",
    "    # Generate synthetic dataset.\n",
    "    num_synthetic_samples = 100\n",
    "    synth_dataset = SyntheticDataset(base_image, base_query_points,\n",
    "                                     num_samples=num_synthetic_samples,\n",
    "                                     num_frames=num_aug_frames)\n",
    "    # Set batch_size to a value >1 if desired.\n",
    "    synth_loader = DataLoader(synth_dataset, batch_size=3, shuffle=False)\n",
    "    \n",
    "    # --- Fine-Tuning Section ---\n",
    "    model = torch.hub.load('./co-tracker', 'cotracker3_offline', source='local').to(device)\n",
    "    print(\"Model Loaded!\")\n",
    "    \n",
    "    reload_model = False # reload model from previous training\n",
    "    if reload_model:\n",
    "        state_dict = torch.load(\"./model_saved/model_dummy.pth\", map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Reloaded previous fine-tuned model.\")\n",
    "    \n",
    "    model.train()\n",
    "    # Freeze all parameters first.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # Unfreeze parameters starting from index 44.\n",
    "    #for i, param in enumerate(model.parameters()):\n",
    "    #    param.requires_grad = (i >= 44)\n",
    "    #    # param.requires_grad = (i >= 10)\n",
    "    \n",
    "    print_model_summary(model)\n",
    "    \n",
    "    # Control scheduler usage here.\n",
    "    use_scheduler = False  # Set to True to enable scheduler, False to disable.\n",
    "    optimizer, scheduler = fetch_optimizer(model, lr=1e-5, weight_decay=1e-5, num_steps=200000, use_scheduler=use_scheduler)\n",
    "    num_epochs = 5\n",
    "    \n",
    "    use_autocast = True\n",
    "    use_gradscaler = True\n",
    "    \n",
    "    print(\"Starting Fine-Tuning...\")\n",
    "    tic = time.time()\n",
    "    model = train_finetune_updated(model, synth_loader, optimizer, scheduler, device, num_epochs=num_epochs,\n",
    "                                   use_autocast=use_autocast, use_gradscaler=use_gradscaler, early_stop_patience=200)\n",
    "    toc = time.time()\n",
    "    print(f\"Elapsed time: {toc - tic:.6f} seconds\")\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"./model_saved/cotracker3_finetuned_AWS_v1SYN_fullFNET_100samplesEPOCH25.pth\")\n",
    "    print(\"Fine-tuning completed and model saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d59be-469c-47c8-9ad9-a0e14eb3d22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
